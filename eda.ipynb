{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MovieLens Preprocessing: Build cleaned_movies.csv\n\nThis notebook loads the MovieLens small dataset, cleans movie titles and genres, computes per-movie average ratings and normalized scores, joins TMDb IDs for posters, and saves a compact `cleaned_movies.csv` for the Streamlit hybrid recommender.\n\nDatasets used (from `ml-latest-small 2/`):\n- `movies.csv` (movieId, title, genres)\n- `ratings.csv` (userId, movieId, rating, timestamp)\n- `links.csv` (movieId, imdbId, tmdbId)\n\nOutput:\n- `/workspace/cleaned_movies.csv` with columns: `movieId`, `tmdbId`, `title` (lowercase), `genres_text`, `avg_rating`, `norm_rating`, `rating_count`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('ml-latest-small 2')\n",
        "MOVIES_CSV = DATA_DIR / 'movies.csv'\n",
        "RATINGS_CSV = DATA_DIR / 'ratings.csv'\n",
        "LINKS_CSV = DATA_DIR / 'links.csv'\n",
        "OUTPUT_CSV = Path('cleaned_movies.csv')\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "movies = pd.read_csv(MOVIES_CSV)\n",
        "ratings = pd.read_csv(RATINGS_CSV)\n",
        "links = pd.read_csv(LINKS_CSV)\n",
        "movies.head(), ratings.head(), links.head()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean titles and genres\n",
        "movies['title'] = movies['title'].fillna('').str.strip().str.lower()\n",
        "movies['genres'] = movies['genres'].fillna('(no genres listed)')\n",
        "# Split and normalize genres\n",
        "movies['genres_list'] = movies['genres'].str.split('|')\n",
        "movies['genres_list'] = movies['genres_list'].apply(lambda lst: [g.strip().lower().replace('-', ' ') for g in lst] if isinstance(lst, list) else [])\n",
        "movies['genres_text'] = movies['genres_list'].apply(lambda lst: ' '.join(sorted(set(lst))))\n",
        "movies[['movieId','title','genres_text']].head()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute average rating and counts per movie\n",
        "ratings = ratings.dropna(subset=['movieId','rating'])\n",
        "agg = ratings.groupby('movieId').agg(avg_rating=('rating','mean'), rating_count=('rating','size')).reset_index()\n",
        "# Normalize avg_rating between 0 and 1\n",
        "if not agg.empty:\n",
        "    min_r, max_r = agg['avg_rating'].min(), agg['avg_rating'].max()\n",
        "    if max_r > min_r:\n",
        "        agg['norm_rating'] = (agg['avg_rating'] - min_r) / (max_r - min_r)\n",
        "    else:\n",
        "        agg['norm_rating'] = 0.5\n",
        "else:\n",
        "    agg['norm_rating'] = pd.Series(dtype=float)\n",
        "agg.head()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Merge datasets\n",
        "df = movies.merge(links[['movieId','tmdbId']], on='movieId', how='left')\n",
        "df = df.merge(agg, on='movieId', how='left')\n",
        "# Fill missing ratings\n",
        "df['avg_rating'] = df['avg_rating'].fillna(0.0)\n",
        "df['norm_rating'] = df['norm_rating'].fillna(0.0)\n",
        "df['rating_count'] = df['rating_count'].fillna(0).astype(int)\n",
        "# Clean tmdbId to int where possible\n",
        "def to_int_or_none(x):\n",
        "    try:\n",
        "        xi = int(x)\n",
        "        return xi if xi > 0 else np.nan\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "df['tmdbId'] = df['tmdbId'].apply(to_int_or_none)\n",
        "# Select columns and save\n",
        "out_cols = ['movieId','tmdbId','title','genres_text','avg_rating','norm_rating','rating_count']\n",
        "cleaned = df[out_cols].copy()\n",
        "cleaned.to_csv(OUTPUT_CSV, index=False)\n",
        "cleaned.head(10)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can run the Streamlit app which loads `cleaned_movies.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualization: Top Genres (bar) and Rating Distribution (hist)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Ensure `cleaned` exists; if not, read from disk\n",
        "try:\n",
        "    cleaned\n",
        "except NameError:\n",
        "    cleaned = pd.read_csv('cleaned_movies.csv')\n",
        "\n",
        "# Top genres bar plot\n",
        "genres_flat = []\n",
        "for g in cleaned['genres_text'].fillna(''):\n",
        "    genres_flat += str(g).split()\n",
        "\n",
        "if genres_flat:\n",
        "    genre_counts = pd.Series(genres_flat).value_counts().head(10)\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    sns.barplot(x=genre_counts.values, y=genre_counts.index, ax=ax, palette=\"Blues_r\")\n",
        "    ax.set_title('Top 10 Genres')\n",
        "    ax.set_xlabel('Count')\n",
        "    ax.set_ylabel('Genre')\n",
        "    plt.show()\n",
        "\n",
        "# Rating distribution histogram (average rating per movie)\n",
        "fig2, ax2 = plt.subplots(figsize=(7, 4))\n",
        "sns.histplot(cleaned['avg_rating'], bins=20, kde=False, ax=ax2, color=\"#4c72b0\")\n",
        "ax2.set_title('Average Rating Distribution (per Movie)')\n",
        "ax2.set_xlabel('Average Rating')\n",
        "ax2.set_ylabel('Number of Movies')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pairplot of rating metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "try:\n",
        "    cleaned\n",
        "except NameError:\n",
        "    cleaned = pd.read_csv('cleaned_movies.csv')\n",
        "\n",
        "pair_cols = ['avg_rating', 'norm_rating', 'rating_count']\n",
        "subset = cleaned[pair_cols].copy()\n",
        "# Avoid extremely skewed scales by clipping rating_count for nicer plots\n",
        "subset['rating_count_clipped'] = subset['rating_count'].clip(upper=subset['rating_count'].quantile(0.99))\n",
        "\n",
        "sns.pairplot(\n",
        "    subset.rename(columns={'rating_count_clipped': 'rating_count (clipped)'}),\n",
        "    vars=['avg_rating', 'norm_rating', 'rating_count (clipped)'],\n",
        "    diag_kind='hist',\n",
        "    corner=True\n",
        ")\n",
        "plt.suptitle('Pairplot: avg_rating, norm_rating, rating_count', y=1.02)\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train/test split for ratings and save to disk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "\n",
        "# Use cleaned ratings (dropna already applied earlier)\n",
        "try:\n",
        "    ratings\n",
        "except NameError:\n",
        "    import pandas as pd\n",
        "    ratings = pd.read_csv('ml-latest-small 2/ratings.csv')\n",
        "    ratings = ratings.dropna(subset=['movieId','rating'])\n",
        "\n",
        "TRAIN_CSV = Path('ml-latest-small 2') / 'ratings_train.csv'\n",
        "TEST_CSV = Path('ml-latest-small 2') / 'ratings_test.csv'\n",
        "\n",
        "ratings_train, ratings_test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "ratings_train.to_csv(TRAIN_CSV, index=False)\n",
        "ratings_test.to_csv(TEST_CSV, index=False)\n",
        "\n",
        "len(ratings_train), len(ratings_test), TRAIN_CSV, TEST_CSV"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recommender utilities for notebook use\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load cleaned data if not available\n",
        "try:\n",
        "    cleaned\n",
        "except NameError:\n",
        "    cleaned = pd.read_csv('cleaned_movies.csv')\n",
        "\n",
        "# Build TF-IDF on genres_text (content-based)\n",
        "_tf_vec = TfidfVectorizer(stop_words='english')\n",
        "_tfidf = _tf_vec.fit_transform(cleaned['genres_text'].fillna(''))\n",
        "\n",
        "# Build user–item pivot from original ratings\n",
        "ratings_path = 'ml-latest-small 2/ratings.csv'\n",
        "ratings_nb = pd.read_csv(ratings_path, usecols=['userId','movieId','rating'])\n",
        "pivot_nb = ratings_nb.pivot_table(index='userId', columns='movieId', values='rating', aggfunc='mean').fillna(0.0)\n",
        "\n",
        "# Ensure pivot columns align to cleaned movieId order\n",
        "movie_ids = cleaned['movieId']\n",
        "missing_cols = [m for m in movie_ids if m not in pivot_nb.columns]\n",
        "for m in missing_cols:\n",
        "    pivot_nb[m] = 0.0\n",
        "pivot_nb = pivot_nb.loc[:, movie_ids]\n",
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def recommend_content(title: str, k: int = 10) -> pd.DataFrame:\n",
        "    idx_list = cleaned.index[cleaned['title'] == title.lower()].tolist()\n",
        "    if not idx_list:\n",
        "        raise ValueError('Title not found')\n",
        "    idx = idx_list[0]\n",
        "    sims = cosine_similarity(_tfidf[idx], _tfidf).ravel()\n",
        "    df = cleaned.copy()\n",
        "    df['score'] = sims\n",
        "    df = df[df.index != idx]\n",
        "    return df.sort_values('score', ascending=False).head(k)[['title','genres_text','avg_rating','score']]\n",
        "\n",
        "\n",
        "def recommend_item_item(title: str, k: int = 10) -> pd.DataFrame:\n",
        "    idx_list = cleaned.index[cleaned['title'] == title.lower()].tolist()\n",
        "    if not idx_list:\n",
        "        raise ValueError('Title not found')\n",
        "    idx = idx_list[0]\n",
        "    target_vec = pivot_nb.iloc[:, idx].values.astype(float)\n",
        "    M = pivot_nb.values.astype(float)\n",
        "    num = M.T @ target_vec\n",
        "    den = (norm(M, axis=0) * norm(target_vec) + 1e-12)\n",
        "    sims = np.nan_to_num(num / den)\n",
        "    df = cleaned.copy()\n",
        "    df['score'] = sims\n",
        "    df = df[df.index != idx]\n",
        "    return df.sort_values('score', ascending=False).head(k)[['title','genres_text','avg_rating','score']]\n",
        "\n",
        "\n",
        "def recommend_hybrid(title: str, k: int = 10, w_content: float = 0.7, w_cf: float = 0.3) -> pd.DataFrame:\n",
        "    idx_list = cleaned.index[cleaned['title'] == title.lower()].tolist()\n",
        "    if not idx_list:\n",
        "        raise ValueError('Title not found')\n",
        "    idx = idx_list[0]\n",
        "    content = cosine_similarity(_tfidf[idx], _tfidf).ravel()\n",
        "    target_vec = pivot_nb.iloc[:, idx].values.astype(float)\n",
        "    M = pivot_nb.values.astype(float)\n",
        "    num = M.T @ target_vec\n",
        "    den = (norm(M, axis=0) * norm(target_vec) + 1e-12)\n",
        "    item = np.nan_to_num(num / den)\n",
        "    final = w_content * content + w_cf * item\n",
        "    df = cleaned.copy()\n",
        "    df['score'] = final\n",
        "    df = df[df.index != idx]\n",
        "    return df.sort_values('score', ascending=False).head(k)[['title','genres_text','avg_rating','score']]"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build item–item CF pivot on train split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "ratings_train = pd.read_csv('ml-latest-small 2/ratings_train.csv')\n",
        "\n",
        "# Align to cleaned movieIds\n",
        "pivot_train = ratings_train.pivot_table(index='userId', columns='movieId', values='rating', aggfunc='mean').fillna(0.0)\n",
        "movie_ids = cleaned['movieId']\n",
        "missing_cols = [m for m in movie_ids if m not in pivot_train.columns]\n",
        "for m in missing_cols:\n",
        "    pivot_train[m] = 0.0\n",
        "pivot_train = pivot_train.loc[:, movie_ids]\n",
        "\n",
        "# Helper for item similarity from train\n",
        "M_train = pivot_train.values.astype(float)\n",
        "\n",
        "def item_sim_vector_from_train(item_index: int) -> np.ndarray:\n",
        "    target_vec = M_train[:, item_index]\n",
        "    num = M_train.T @ target_vec\n",
        "    den = (norm(M_train, axis=0) * norm(target_vec) + 1e-12)\n",
        "    return np.nan_to_num(num / den)\n",
        "\n",
        "print('Train pivot shape (users x items):', pivot_train.shape)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Try content-based, item-item CF, and hybrid\n",
        "# Choose a title (lowercase) that exists in cleaned['title']\n",
        "example_title = 'toy story (1995)'.lower()\n",
        "\n",
        "try:\n",
        "    print('Content-based:')\n",
        "    display(recommend_content(example_title, k=10))\n",
        "    print('\\nItem-item CF:')\n",
        "    display(recommend_item_item(example_title, k=10))\n",
        "    print('\\nHybrid (0.7 content, 0.3 CF):')\n",
        "    display(recommend_hybrid(example_title, k=10, w_content=0.7, w_cf=0.3))\n",
        "except ValueError as e:\n",
        "    print('Error:', e)\n",
        "    print('Available example titles:', cleaned['title'].head(10).tolist())"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Demo: Item-item CF using TRAIN split\n",
        "example_title = 'toy story (1995)'.lower()\n",
        "idx_list = cleaned.index[cleaned['title'] == example_title].tolist()\n",
        "if idx_list:\n",
        "    idx = idx_list[0]\n",
        "    sims_train = item_sim_vector_from_train(idx)\n",
        "    df = cleaned.copy()\n",
        "    df['score'] = sims_train\n",
        "    display(df[df.index != idx].sort_values('score', ascending=False).head(10)[['title','genres_text','avg_rating','score']])\n",
        "else:\n",
        "    print('Title not found in cleaned data.')"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RMSE evaluation on TEST split (baseline, content, item-item CF, hybrid)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Ensure required data objects exist\n",
        "try:\n",
        "    cleaned\n",
        "except NameError:\n",
        "    cleaned = pd.read_csv('cleaned_movies.csv')\n",
        "\n",
        "ratings_train = pd.read_csv('ml-latest-small 2/ratings_train.csv')\n",
        "ratings_test = pd.read_csv('ml-latest-small 2/ratings_test.csv')\n",
        "\n",
        "# Map movieId to cleaned index (only evaluate items present in cleaned)\n",
        "movieId_to_idx = pd.Series(index=cleaned['movieId'].values, data=np.arange(len(cleaned))).to_dict()\n",
        "\n",
        "# Precompute baselines from TRAIN\n",
        "movie_mean_train = ratings_train.groupby('movieId')['rating'].mean()\n",
        "user_mean_train = ratings_train.groupby('userId')['rating'].mean()\n",
        "global_mean_train = ratings_train['rating'].mean()\n",
        "\n",
        "# Build user->list of (movie_idx, rating) from TRAIN\n",
        "user_train_items = {}\n",
        "for row in ratings_train.itertuples(index=False):\n",
        "    mi = movieId_to_idx.get(row.movieId)\n",
        "    if mi is None:\n",
        "        continue\n",
        "    user_train_items.setdefault(row.userId, []).append((mi, float(row.rating)))\n",
        "\n",
        "# Reuse previously built matrices if present; otherwise build now\n",
        "try:\n",
        "    _tfidf\n",
        "except NameError:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    _tfidf = TfidfVectorizer(stop_words='english').fit_transform(cleaned['genres_text'].fillna(''))\n",
        "\n",
        "try:\n",
        "    pivot_train\n",
        "except NameError:\n",
        "    # Create pivot on TRAIN\n",
        "    pt = ratings_train.pivot_table(index='userId', columns='movieId', values='rating', aggfunc='mean').fillna(0.0)\n",
        "    # Align columns to cleaned movie order\n",
        "    missing_cols = [m for m in cleaned['movieId'] if m not in pt.columns]\n",
        "    for m in missing_cols:\n",
        "        pt[m] = 0.0\n",
        "    pivot_train = pt.loc[:, cleaned['movieId']]\n",
        "\n",
        "# Prepare dense matrix for item vectors and their norms (users x items)\n",
        "M_train = pivot_train.values.astype(float)\n",
        "col_norms = norm(M_train, axis=0) + 1e-12\n",
        "\n",
        "\n",
        "def predict_baseline(user_id: int, movie_idx: int) -> float:\n",
        "    mid = int(cleaned.loc[movie_idx, 'movieId'])\n",
        "    return float(movie_mean_train.get(mid, global_mean_train))\n",
        "\n",
        "\n",
        "def weighted_avg(neigh_idxs, sims, ratings, k=20):\n",
        "    if len(neigh_idxs) == 0:\n",
        "        return np.nan\n",
        "    # Select top-K by similarity magnitude\n",
        "    order = np.argsort(-np.abs(sims))[:k]\n",
        "    sims_k = sims[order]\n",
        "    ratings_k = ratings[order]\n",
        "    denom = np.sum(np.abs(sims_k)) + 1e-12\n",
        "    return float(np.sum(sims_k * ratings_k) / denom)\n",
        "\n",
        "\n",
        "def predict_cf_item_item(user_id: int, movie_idx: int, k: int = 20) -> float:\n",
        "    # Items this user rated in TRAIN\n",
        "    items = user_train_items.get(user_id, [])\n",
        "    if not items:\n",
        "        return np.nan\n",
        "    # Compute cosine sims only to those items\n",
        "    target_vec = M_train[:, movie_idx]\n",
        "    target_norm = col_norms[movie_idx]\n",
        "    sims = []\n",
        "    ratings = []\n",
        "    for j_idx, r in items:\n",
        "        s = float(np.dot(M_train[:, j_idx], target_vec) / (col_norms[j_idx] * target_norm))\n",
        "        sims.append(s)\n",
        "        ratings.append(r)\n",
        "    sims = np.array(sims, dtype=float)\n",
        "    ratings = np.array(ratings, dtype=float)\n",
        "    return weighted_avg(np.arange(len(items)), sims, ratings, k=k)\n",
        "\n",
        "\n",
        "def predict_content(user_id: int, movie_idx: int, k: int = 20) -> float:\n",
        "    items = user_train_items.get(user_id, [])\n",
        "    if not items:\n",
        "        return np.nan\n",
        "    # Cosine sims from the target item to items rated by user using TF-IDF\n",
        "    J = [j for (j, _) in items]\n",
        "    sims_vec = cosine_similarity(_tfidf[movie_idx], _tfidf[J]).ravel()\n",
        "    ratings = np.array([r for (_, r) in items], dtype=float)\n",
        "    return weighted_avg(np.arange(len(J)), sims_vec, ratings, k=k)\n",
        "\n",
        "\n",
        "def predict_hybrid(user_id: int, movie_idx: int, k: int = 20, w_content: float = 0.7, w_cf: float = 0.3) -> float:\n",
        "    p_c = predict_content(user_id, movie_idx, k=k)\n",
        "    p_cf = predict_cf_item_item(user_id, movie_idx, k=k)\n",
        "    if np.isnan(p_c) and np.isnan(p_cf):\n",
        "        return np.nan\n",
        "    if np.isnan(p_c):\n",
        "        return p_cf\n",
        "    if np.isnan(p_cf):\n",
        "        return p_c\n",
        "    return float(w_content * p_c + w_cf * p_cf)\n",
        "\n",
        "\n",
        "def compute_rmse(method: str, n_samples: int | None = 5000, k: int = 20):\n",
        "    # Create evaluation set limited to items in cleaned\n",
        "    rows = []\n",
        "    for row in ratings_test.itertuples(index=False):\n",
        "        mi = movieId_to_idx.get(row.movieId)\n",
        "        if mi is None:\n",
        "            continue\n",
        "        rows.append((int(row.userId), mi, float(row.rating)))\n",
        "    if n_samples is not None and len(rows) > n_samples:\n",
        "        rows = rows[:n_samples]\n",
        "    preds = []\n",
        "    trues = []\n",
        "    for user_id, movie_idx, true_r in rows:\n",
        "        if method == 'baseline':\n",
        "            p = predict_baseline(user_id, movie_idx)\n",
        "        elif method == 'content':\n",
        "            p = predict_content(user_id, movie_idx, k=k)\n",
        "        elif method == 'itemcf':\n",
        "            p = predict_cf_item_item(user_id, movie_idx, k=k)\n",
        "        elif method == 'hybrid':\n",
        "            p = predict_hybrid(user_id, movie_idx, k=k)\n",
        "        else:\n",
        "            raise ValueError('Unknown method')\n",
        "        if np.isnan(p):\n",
        "            p = predict_baseline(user_id, movie_idx)\n",
        "        preds.append(p)\n",
        "        trues.append(true_r)\n",
        "    preds = np.array(preds, dtype=float)\n",
        "    trues = np.array(trues, dtype=float)\n",
        "    rmse = float(np.sqrt(np.mean((preds - trues) ** 2)))\n",
        "    return rmse, len(rows)\n",
        "\n",
        "print('Computing RMSE (may take a moment)...')\n",
        "for method in ['baseline', 'content', 'itemcf', 'hybrid']:\n",
        "    rmse, n = compute_rmse(method, n_samples=5000, k=20)\n",
        "    print(f\"{method} RMSE (n={n}): {rmse:.4f}\")"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}